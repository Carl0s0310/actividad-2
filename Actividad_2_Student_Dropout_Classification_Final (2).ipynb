{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "titulo",
      "metadata": {},
      "source": [
        "# Actividad 2 - Student Dropout Classification (Versión Final )\n",
        "\n",
        "**Integrantes:**  \n",
        "- Hernando Luis Calvo Ochoa  \n",
        "- Carlos Antonio Ardila Ruiz  \n",
        "\n",
        "**Instrucciones:** Este notebook realiza limpieza, EDA, análisis de correlación, preprocesamiento (ColumnTransformer), entrenamiento y comparación de Regresión Logística y Árbol de Decisión. Está listo para ejecutarse en Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "id": "importaciones",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cargar librerías\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_curve, auc"
      ]
    },
    {
      "cell_type": "code",
      "id": "cargar_datos",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cargar dataset\n",
        "csv_path = '/mnt/data/student_dropout.csv'  # cambiar la ruta según corresponda\n",
        "df = pd.read_csv(csv_path)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "id": "resumen_eda",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Resumen básico del dataset\n",
        "print('Shape:', df.shape)\n",
        "print('\\nColumnas:', df.columns.tolist())\n",
        "print('\\nValores nulos por columna:\\n', df.isnull().sum())\n",
        "print('\\nConteo de clases originales:\\n', df['Target'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "id": "mapa_correlaciones",
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Análisis de correlación ===\n",
        "corr = df.select_dtypes(include=[np.number]).corr()\n",
        "plt.figure(figsize=(10,8))\n",
        "sns.heatmap(corr, annot=True, cmap='coolwarm', fmt='.2f')\n",
        "plt.title('Mapa de Correlaciones entre Variables Numéricas')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "id": "preparar_datos",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mapear Target a binario (Dropout=1, else=0)\n",
        "df['dropout_flag'] = (df['Target'] == 'Dropout').astype(int)\n",
        "X = df.drop(columns=['Target','dropout_flag'])\n",
        "y = df['dropout_flag']\n",
        "print('Distribución binaria de la variable objetivo:')\n",
        "print(y.value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "id": "columnas",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Identificar columnas numéricas y categóricas\n",
        "num_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
        "cat_cols = X.select_dtypes(include=['object','category','bool']).columns.tolist()\n",
        "print('Columnas numéricas:', num_cols)\n",
        "print('Columnas categóricas:', cat_cols)"
      ]
    },
    {
      "cell_type": "code",
      "id": "preprocesamiento",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preprocesamiento: imputación y escalado para numéricas; imputación y OneHot para categóricas\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "    ('num', numeric_transformer, num_cols),\n",
        "    ('cat', categorical_transformer, cat_cols)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "id": "split",
      "metadata": {},
      "outputs": [],
      "source": [
        "# División entrenamiento/prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
        "print('Train/Test shapes:', X_train.shape, X_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "id": "modelos",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Modelos con pipeline\n",
        "pipe_lr = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', LogisticRegression(max_iter=1000))])\n",
        "pipe_dt = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', DecisionTreeClassifier(random_state=42))])"
      ]
    },
    {
      "cell_type": "code",
      "id": "validacion",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Validación cruzada (5-fold, F1-score)\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "scores_lr = cross_val_score(pipe_lr, X_train, y_train, cv=cv, scoring='f1')\n",
        "scores_dt = cross_val_score(pipe_dt, X_train, y_train, cv=cv, scoring='f1')\n",
        "print('Regresión Logística - F1 medio:', scores_lr.mean(), '+/-', scores_lr.std())\n",
        "print('Árbol de Decisión - F1 medio:', scores_dt.mean(), '+/-', scores_dt.std())"
      ]
    },
    {
      "cell_type": "code",
      "id": "evaluacion",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Entrenar y evaluar en test\n",
        "pipe_lr.fit(X_train, y_train)\n",
        "pipe_dt.fit(X_train, y_train)\n",
        "\n",
        "y_pred_lr = pipe_lr.predict(X_test)\n",
        "y_pred_dt = pipe_dt.predict(X_test)\n",
        "y_proba_lr = pipe_lr.predict_proba(X_test)[:,1]\n",
        "y_proba_dt = pipe_dt.predict_proba(X_test)[:,1]\n",
        "\n",
        "for name, y_pred, y_proba in [('Regresión Logística', y_pred_lr, y_proba_lr), ('Árbol de Decisión', y_pred_dt, y_proba_dt)]:\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    prec = precision_score(y_test, y_pred, zero_division=0)\n",
        "    rec = recall_score(y_test, y_pred, zero_division=0)\n",
        "    f1 = f1_score(y_test, y_pred, zero_division=0)\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    print('\\n==', name, '==')\n",
        "    print('Accuracy:', acc)\n",
        "    print('Precision:', prec)\n",
        "    print('Recall:', rec)\n",
        "    print('F1:', f1)\n",
        "    print('ROC AUC:', roc_auc)\n",
        "    print('Confusion Matrix:\\n', cm)\n",
        "\n",
        "    # Gráfico ROC\n",
        "    plt.plot(fpr, tpr, label=f'{name} (AUC={roc_auc:.2f})')\n",
        "\n",
        "plt.plot([0,1],[0,1],'--',color='gray')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Curvas ROC')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "explicacion_modelos",
      "metadata": {},
      "source": [
        "## Explicación de los modelos\n",
        "\n",
        "**Regresión Logística:**\n",
        "Modelo estadístico lineal que estima la probabilidad de que un estudiante abandone. Usa una función *sigmoide* para convertir combinaciones lineales de las variables en valores entre 0 y 1. Es útil para interpretar la influencia de cada factor (por ejemplo, si la nota del primer semestre disminuye, la probabilidad de abandono aumenta).\n",
        "\n",
        "**Árbol de Decisión:**\n",
        "Modelo no lineal que divide los datos en ramas según condiciones lógicas (por ejemplo, `promedio < 3.0`). Cada división intenta separar mejor las clases. Es intuitivo y fácil de visualizar, pero puede sobreajustarse si no se controla su profundidad.\n",
        "\n",
        "Ambos modelos son apropiados para un sistema de alerta temprana, donde la prioridad es detectar estudiantes en riesgo."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "conclusion_final",
      "metadata": {},
      "source": [
        "## Conclusiones\n",
        "\n",
        "- Ambos modelos presentan buen desempeño, pero la **Regresión Logística** suele tener un F1 más alto y mayor interpretabilidad.\n",
        "- Si el objetivo es **detectar el mayor número posible de estudiantes en riesgo**, se puede ajustar el umbral de probabilidad para aumentar el *recall*.\n",
        "- Futuras mejoras: aplicar técnicas de *balanceo de clases*, selección de variables y probar modelos más complejos (Random Forest, XGBoost).\n"
      ]
    }
  ],
  "metadata": {},
  "nbformat": 4,
  "nbformat_minor": 5
}